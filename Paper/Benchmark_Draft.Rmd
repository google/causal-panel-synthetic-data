---
title: "Benchmarking Time-Series Cross Sectional Methods: Synthetic Data Approach"
#author: Alex Kellogg, Qing Wu
#output: html_document
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Measuring the causal impact of an intervention has long been a central interest for econometricians, and has more recently become a focus in industry, where large scale experiments are frequently run to inform strategic decisions. In this world, the data often contain a large time series component on the outcome of interest, with few relevant covariates, staggered treatment times, and many more control/donor units than treated. Typical empirical applications rely on panel models such as the Synthetic Control Method - henceforth SCM - (Abadie, Diamond, and Hainmueller, 2010; 2015) which impute the unobserved potential outcome for the treated units and compute the Average Treatment Effect on the Treated (ATT) as the average difference between observed and counterfactual outcomes. 

The original formulation of SCM predicted counterfactual outcomes as a convex combination of donor units, with weights determined by goodness-of-fit on lagged outcome and auxiliary covariates. However, a number of alternatives have since been proposed, including the introduction of time weights as well as unit weights in Athey et al. (2019)'s SCDID, or Ben-Michael et al (2019)'s bias corrected Augmented Synthetic Control Method (ASCM). More generally, researchers have suggested numerous ways to optimally impute counterfactual outcomes, ranging from Matrix Completion (MC) methods common in computer science (Athey et al. 2018), modeling the outcome as composed of latent factors and estimating these time-varying factors and individual specific loadings (Bai, 2009; Gobillon and Magnac, 2016; Xu, 2017), or using Bayesian Structural Time Series models, as in CausalImpact (Scott and Varian, 2014; Brodersen et al, 2015).

While this expansive and expanding toolkit provides practitioners with an abundance of choice, it can be hard for practicioners to determine which method is most suitable for their specific purpose. In this paper, our goal is to provide a systematic empirical comparison of these methods as well as an R package intended to guide users to methodologies most suited to their data. Our Monte Carlo comparison relies on a common synthetic DGP framework nesting selection into treatment (random assignment, selection on observables, selection on unobservables), overlap in treated and control time series, heterogeneity in treatment impact and decay, and heterogeneity in the auto-correlation of outcomes. Because the number of potential models is quite large, we focus on a subset primarily based on existing package infrastructure in R -- namely CausalImpact, Gsynth, Matrix Completion, and SCDID. 

Several papers provide a starting point for our analysis, most notably Gobillon and Magnac (2016) who conduct Monte Carlo experiments in a similar envirionment, comparing several variations of Bai (2009)'s IFE model^[These include standard IFE to compute counterfactual, IFE with a treatment dummy, IFE with counterfactuals based on matches from estimated factor loadings, IFE with counterfactual constrained to SCM weights based on factor loadings], as well as SCM and standard Difference-in-differences. They conclude that each method is unbiased in their baseline case with random assignment to treatment, but upon introducing correlations between loadings and treatment assignment, all methods become severely biased except for standard IFE and IFE with a treatment dummy^[Difference-in-differences is also unbiased here, but the authors demonstrate that this is an artifact of their main DGP.].

In a similar vein, Xu (2017) reports results comparing IFE and SCM to their proposed GSC (Gsynth) estimator, which first estimates latent factors using donor units, then estimates factor loadings for each treated observation, and ultimately combines the two to compute counterfactual. The Monte Carlo experiments, using a modified version of the DGP in Bai (2009) and Gobillon and Magnac (2016), suggest that the Gsynth method is less biased than traditional difference-in-differences when there exist unobservable, decomposable time-varying confounders, and is less biased than IFE under heterogeneous treatment effects. Lastly, they show that Gsynth tends to be more efficient than SCM, and demonstrate the robustness of these results by recomputing the metrics using cross-validation to estimate the number of factors instead of assuming the correct model specification.

Finally, Gardeazabal and Vega-Bayo (2017) sample from existing data as well as their synthetic DGP to compare SCM to a panel data approach (IFE) by Hsiao et al (2012) under different circumstances. Interestingly, they explore how robust the methodologies are to changes in the donor pool -- selecting the subsample of donors from those that have positive weights according to SCM or those that are statististically significant under the IFE approach -- and conclude that SCM is more robust to these alterations. However, in simulation studies focusing on cases with bad pre-treatment matches (defined by Mean Absolute Error, MAE), the results suggest that SCM is much more biased, while IFE models are unbiased but have large confidence intervals.

Our benchmarking exercise adds to this existing literature in several ways. First, we provide a more expansive set of comparisons by introducing generalized models on a common DGP; SCDID generalizates both Difference-in-differences and SCM; the Gsynth package (Xu, 2017) provides implementations for one or two-way fixed effects, IFE using EM (Gobillon and Magnac, 2016), Matrix Completion (Athey et al 2018), as well as the Gsynth algorithm; CausalImpact covers a large number of time series models under the umbrella of BSTS (including ARIMA models). Second, the flexibility of our DGP allows us to examine a number of interesting cases with the change of input parameters. Third, inspired by an expansive literature on forecasting time-series, we provide a simple characterization of the data using time-series features such as ACF and entropy, and analyze whether differences in these features are predictive of differences in methodology performance (Kang et al 2017). Fourth, borrowing from machine learning, we study whether ensembles can provide performance boosts in this context (Hastie et al, 2013; Athey et al, 2019).

The rest of the paper proceeds as follows.  Section \@ref(sec:methods) provides a brief overview of each of the methods we analyze herein, with appropriate references for further information. Section \@ref(sec:dgp) provides an annotated walkthrough our DGP, which serves as the basis for all of our MCMC simulations.


# Methods Overview {#sec:methods}
## Notation
Throughout the paper, we refer to individuals as $i \in (1,2,\dots,N)$ and time periods as $t \in (1,2, \dots, T)$. A subset of the $N$ units are treated at varying times, and once treated, remain so for the duration. Thus, each unit can be decomposed into treated ($N_{tr}$) and control ($N_c$) such that $N=N_{tr}+N_c$. The total number of time periods, $T$, can be similarly decomposed into the pre-treatment periods, $t\in(1, 2, \dots, T_0)$, and post-treatment periods ($t \in (T_0+1,T_0+2, \dots, T)$) given the time $T_0+1$ that unit $i$ is first treated. Let $D_{it}$ represent whether unit $i$ received treatment in time $t$, so $D_{it}=1$ if $i \in N_{tr}, t \geq T_{0_i}$, and $0$ otherwise. Potential outcomes for unit $i$ at time $t$ under control and treatment respectively are $Y_{it}(0), Y_{it}(1)$ while observed outcomes are \begin{align*}
Y_{it} = \begin{cases}
Y_{it}(0) &D_{it}=0\\
Y_{it}(1) &D_{it}=1,
\end{cases}
\end{align*} which we can rewrite in the Rubin causal framework as $Y_{it}=(1-D_{it})Y_{it}(0)+D_{it}Y_{it}(1)$ (Rubin, 1974).

# DGP {#sec:dgp}

In this section, we outline the DGP used as the basis of our benchmarking exercises. We begin by giving a highlighting the key features of our DGP in a high-level overview, and then go into code and details in a separate subsection for interested readers.

## Key Features

Our data is generating from the following factor augmented autoregressive process of order one:

\begin{align*}
ln Y_{it}(0)&=\alpha_i+\rho_ilnY_{i,t-1}(0)+F_t\lambda_i'+\epsilon_{it}\\
ln Y_{it}(1)&=lnY_{it}(0)+D_{it}\tau_{it}.
\end{align*}

Each unit has an individual specific intercept ($\alpha_i$), auto-correlation coefficient ($\rho_i$),  as well as factor loadings, $(\lambda_{i}$, dimension $1 \times r$, where $r$ is the number of unobserved factors) which scale unobserved, time-varying factors $(F_t$, dimension $1 \times r$ for a given $t$). The noise term, $\epsilon_{it}$ is iid $\mathcal{N}\sim(0,0.5^2)$. Finally, treatment impact is modeled as a shock to counterfactual outcomes, with our binary indicator $D_{it}$ encoding treatment status and $\tau_{it}$ representing the impact of treatment for unit $i$ at time $t$.

We allow for a number of treatment assignment mechanisms, such as random assignment ($E(\alpha_i|D_{it}=1)=E(\alpha_i|D_{it}=0)$ as well as for $\rho_i, \lambda_i$), selection on observables ($D_{it}$ is correlated with at least one of $\alpha_i, \rho_i, \lambda_i$), and selection on unobservables ^[For now, the outcome is actually not effected by additional observables or unobservables. However, these variables have been created, selection on them is modeled, and the outcome can be made to depend on them by modeling $\alpha_i=a_i+\beta_{obs} X_{1i}+\beta_{unobs}X_{2i}$. ]. Extreme observations can also be made more frequent by tweaking an overlap parameter, which shifts the distribution of covariates^[For now, we just shift observable and unobservable $X$, which do not interact with $Y$, though we should easily be able to shift $\rho, \lambda$.] for a fraction of observations, and then randomly assigns treatment.^[I can't tell if this is helpful above and beyond traditional selection.]

Furthermore, we allow for varying degrees of heterogeneity in both treatment impact and decay. Our treatment effect $\tau_{it}=\omega_{i}*\delta_{it}$ flexibly covers many cases, including impact decay by setting (for example) $\delta_{it}=\delta_i^{t-T_{0i}}$ -- where $\delta_i$ itself can display heterogeneity -- as well as impact heterogeneity via $\omega_i$. 

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
